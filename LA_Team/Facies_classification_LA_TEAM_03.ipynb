{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facies classification using Machine Learning #\n",
    "## LA Team Submission 3 ## \n",
    "### _[Lukas Mosser](https://at.linkedin.com/in/lukas-mosser-9948b32b/en), [Alfredo De la Fuente](https://pe.linkedin.com/in/alfredodelafuenteb)_ ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this python notebook we explore a facies classification model using Deep Neural Networks taking into account spatial dependencies to outperform the prediction model proposed in the [prediction facies from wel logs challenge](https://github.com/seg/2016-ml-contest). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Modeling\n",
    "----\n",
    "\n",
    "The dataset we will use comes from a class excercise from The University of Kansas on [Neural Networks and Fuzzy Systems](http://www.people.ku.edu/~gbohling/EECS833/).  This exercise is based on a consortium project to use machine learning techniques to create a reservoir model of the largest gas fields in North America, the Hugoton and Panoma Fields. For more info on the origin of the data, see [Bohling and Dubois (2003)](http://www.kgs.ku.edu/PRS/publication/2003/ofr2003-50.pdf) and [Dubois et al. (2007)](http://dx.doi.org/10.1016/j.cageo.2006.08.011). \n",
    "\n",
    "The dataset we will use is log data from nine wells that have been labeled with a facies type based on oberservation of core.  We will use this log data to train a classifier to predict facies types. \n",
    "\n",
    "This data is from the Council Grove gas reservoir in Southwest Kansas.  The Panoma Council Grove Field is predominantly a carbonate gas reservoir encompassing 2700 square miles in Southwestern Kansas.  This dataset is from nine wells (with 4149 examples), consisting of a set of seven predictor variables and a rock facies (class) for each example vector and validation (test) data (830 examples from two wells) having the same seven predictor variables in the feature vector.  Facies are based on examination of cores from nine wells taken vertically at half-foot intervals. Predictor variables include five from wireline log measurements and two geologic constraining variables that are derived from geologic knowledge. These are essentially continuous variables sampled at a half-foot sample rate. \n",
    "\n",
    "The seven predictor variables are:\n",
    "* Five wire line log curves include [gamma ray](http://petrowiki.org/Gamma_ray_logs) (GR), [resistivity logging](http://petrowiki.org/Resistivity_and_spontaneous_%28SP%29_logging) (ILD_log10),\n",
    "[photoelectric effect](http://www.glossary.oilfield.slb.com/en/Terms/p/photoelectric_effect.aspx) (PE), [neutron-density porosity difference and average neutron-density porosity](http://petrowiki.org/Neutron_porosity_logs) (DeltaPHI and PHIND). Note, some wells do not have PE.\n",
    "* Two geologic constraining variables: nonmarine-marine indicator (NM_M) and relative position (RELPOS)\n",
    "\n",
    "The nine discrete facies (classes of rocks) are: \n",
    "1. Nonmarine sandstone\n",
    "2. Nonmarine coarse siltstone \n",
    "3. Nonmarine fine siltstone \n",
    "4. Marine siltstone and shale \n",
    "5. Mudstone (limestone)\n",
    "6. Wackestone (limestone)\n",
    "7. Dolomite\n",
    "8. Packstone-grainstone (limestone)\n",
    "9. Phylloid-algal bafflestone (limestone)\n",
    "\n",
    "These facies aren't discrete, and gradually blend into one another. Some have neighboring facies that are rather close.  Mislabeling within these neighboring facies can be expected to occur.  The following table lists the facies, their abbreviated labels and their approximate neighbors.\n",
    "\n",
    "Facies |Label| Adjacent Facies\n",
    ":---: | :---: |:--:\n",
    "1 |SS| 2\n",
    "2 |CSiS| 1,3\n",
    "3 |FSiS| 2\n",
    "4 |SiSh| 5\n",
    "5 |MS| 4,6\n",
    "6 |WS| 5,7\n",
    "7 |D| 6,8\n",
    "8 |PS| 6,7,9\n",
    "9 |BS| 7,8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "---\n",
    "\n",
    "Let's import all the libraries that will be particularly needed for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): pandas in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): python-dateutil in /home/alfredo/anaconda2/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pytz>=2011k in /home/alfredo/anaconda2/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.7.0 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.5 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from python-dateutil->pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scikit-learn in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): keras in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): pyyaml in /home/alfredo/anaconda2/lib/python2.7/site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): theano in /home/alfredo/anaconda2/lib/python2.7/site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six in /home/alfredo/anaconda2/lib/python2.7/site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.7.1 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from theano->keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy>=0.11 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from theano->keras)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "pip install pandas\n",
    "pip install scikit-learn\n",
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold , StratifiedKFold\n",
    "from classification_utilities import display_cm, display_adj_cm\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training and testing data to preprocess it for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Facies</th>\n",
       "      <th>Formation</th>\n",
       "      <th>Well Name</th>\n",
       "      <th>Depth</th>\n",
       "      <th>GR</th>\n",
       "      <th>ILD_log10</th>\n",
       "      <th>DeltaPHI</th>\n",
       "      <th>PHIND</th>\n",
       "      <th>PE</th>\n",
       "      <th>NM_M</th>\n",
       "      <th>RELPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2793.0</td>\n",
       "      <td>77.45</td>\n",
       "      <td>0.664</td>\n",
       "      <td>9.9</td>\n",
       "      <td>11.915</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2793.5</td>\n",
       "      <td>78.26</td>\n",
       "      <td>0.661</td>\n",
       "      <td>14.2</td>\n",
       "      <td>12.565</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2794.0</td>\n",
       "      <td>79.05</td>\n",
       "      <td>0.658</td>\n",
       "      <td>14.8</td>\n",
       "      <td>13.050</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2794.5</td>\n",
       "      <td>86.10</td>\n",
       "      <td>0.655</td>\n",
       "      <td>13.9</td>\n",
       "      <td>13.115</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2795.0</td>\n",
       "      <td>74.58</td>\n",
       "      <td>0.647</td>\n",
       "      <td>13.5</td>\n",
       "      <td>13.300</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2795.5</td>\n",
       "      <td>73.97</td>\n",
       "      <td>0.636</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.385</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2796.0</td>\n",
       "      <td>73.72</td>\n",
       "      <td>0.630</td>\n",
       "      <td>15.6</td>\n",
       "      <td>13.930</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2796.5</td>\n",
       "      <td>75.65</td>\n",
       "      <td>0.625</td>\n",
       "      <td>16.5</td>\n",
       "      <td>13.920</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2797.0</td>\n",
       "      <td>73.79</td>\n",
       "      <td>0.624</td>\n",
       "      <td>16.2</td>\n",
       "      <td>13.980</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2797.5</td>\n",
       "      <td>76.89</td>\n",
       "      <td>0.615</td>\n",
       "      <td>16.9</td>\n",
       "      <td>14.220</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Facies Formation  Well Name   Depth     GR  ILD_log10  DeltaPHI   PHIND  \\\n",
       "0       3     A1 SH  SHRIMPLIN  2793.0  77.45      0.664       9.9  11.915   \n",
       "1       3     A1 SH  SHRIMPLIN  2793.5  78.26      0.661      14.2  12.565   \n",
       "2       3     A1 SH  SHRIMPLIN  2794.0  79.05      0.658      14.8  13.050   \n",
       "3       3     A1 SH  SHRIMPLIN  2794.5  86.10      0.655      13.9  13.115   \n",
       "4       3     A1 SH  SHRIMPLIN  2795.0  74.58      0.647      13.5  13.300   \n",
       "5       3     A1 SH  SHRIMPLIN  2795.5  73.97      0.636      14.0  13.385   \n",
       "6       3     A1 SH  SHRIMPLIN  2796.0  73.72      0.630      15.6  13.930   \n",
       "7       3     A1 SH  SHRIMPLIN  2796.5  75.65      0.625      16.5  13.920   \n",
       "8       3     A1 SH  SHRIMPLIN  2797.0  73.79      0.624      16.2  13.980   \n",
       "9       3     A1 SH  SHRIMPLIN  2797.5  76.89      0.615      16.9  14.220   \n",
       "\n",
       "    PE  NM_M  RELPOS  \n",
       "0  4.6     1   1.000  \n",
       "1  4.1     1   0.979  \n",
       "2  3.6     1   0.957  \n",
       "3  3.5     1   0.936  \n",
       "4  3.4     1   0.915  \n",
       "5  3.6     1   0.894  \n",
       "6  3.7     1   0.872  \n",
       "7  3.5     1   0.830  \n",
       "8  3.4     1   0.809  \n",
       "9  3.5     1   0.787  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'train_test_data.csv'\n",
    "data = pd.read_csv(filename)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill the missing data values in the PE field with zero and proceed to normalize the data that will be fed into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set 'Well Name' and 'Formation' fields as categories\n",
    "data['Well Name'] = data['Well Name'].astype('category')\n",
    "data['Formation'] = data['Formation'].astype('category')\n",
    "\n",
    "# Fill missing values and normalize for 'PE' field\n",
    "data['PE'] = data['PE'].fillna(value=0)\n",
    "mean_pe = data['PE'].mean()\n",
    "std_pe = data['PE'].std()\n",
    "data['PE'] = (data['PE']-mean_pe)/std_pe\n",
    "\n",
    "# Normalize the rest of fields (GR, ILD_log10, DelthaPHI, PHIND,NM_M,RELPOS)\n",
    "correct_facies_labels = data['Facies'].values\n",
    "feature_vectors = data.drop(['Formation', 'Depth'], axis=1)\n",
    "well_labels = data[['Well Name', 'Facies']].values\n",
    "data_vectors = feature_vectors.drop(['Well Name', 'Facies'], axis=1).values\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(data_vectors)\n",
    "scaled_features = scaler.transform(data_vectors)\n",
    "data_out = np.hstack([well_labels, scaled_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start training stage, it is required to format the data by considering a sliding window over the depth component in order to classify a given set of features at some specific depth for each well in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data_out):\n",
    "    \n",
    "    data = data_out\n",
    "    well_data = {}\n",
    "    well_names = list(set(data[:, 0]))\n",
    "    for name in well_names:\n",
    "        well_data[name] = [[], []]\n",
    "\n",
    "    for row in data:\n",
    "        well_data[row[0]][1].append(row[1])\n",
    "        well_data[row[0]][0].append(list(row[2::]))\n",
    "\n",
    "    # Sliding window\n",
    "    positive_lag = 0\n",
    "    negative_lag = 0\n",
    "\n",
    "    chunks = []\n",
    "    chunks_test = []\n",
    "    chunk_length = positive_lag+negative_lag+1 \n",
    "    chunks_facies = []\n",
    "    for name in well_names:\n",
    "        if name not in ['STUART', 'CRAWFORD']:\n",
    "            test_well_data = well_data[name]\n",
    "            log_values = np.array(test_well_data[0])\n",
    "            facies_values =  np.array(test_well_data[1])\n",
    "            for i in range(log_values.shape[0]):\n",
    "                chunks.append(log_values[i:i+1, :])\n",
    "                chunks_facies.append(facies_values[i])\n",
    "        else:\n",
    "            test_well_data = well_data[name]\n",
    "            log_values = np.array(test_well_data[0])\n",
    "            facies_values =  np.array(test_well_data[1])\n",
    "            for i in range(log_values.shape[0]):\n",
    "                chunks_test.append(log_values[i:i+1, :])\n",
    "    \n",
    "    chunks_facies = np.array(chunks_facies, dtype=np.int32)-1\n",
    "    X_ = np.array(chunks)\n",
    "    X = np.zeros((len(X_),len(X_[0][0]) * len(X_[0])))\n",
    "    for i in range(len(X_)):\n",
    "        X[i,:] = X_[i].flatten()\n",
    "        \n",
    "    X_test = np.array(chunks_test)\n",
    "    X_test_out = np.zeros((len(X_test),len(X_test[0][0]) * len(X_test[0])))\n",
    "    for i in range(len(X_test)):\n",
    "        X_test_out[i,:] = X_test[i].flatten()\n",
    "    y = np_utils.to_categorical(chunks_facies)\n",
    "    return X, y, X_test_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "---\n",
    "\n",
    "We will experiment with an ensemble of classification models to outperform the accuracy at predicting facies. As input we will consider a set of features extracted by padding a depth interval segment, that way we take into account spatial dependencies. As output we obtain a vector filled with values ranging from [0-8] that indicate the presence of each facie with respect to depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "np.random.seed(7) \n",
    "# Load data\n",
    "X_train, y_train, X_test = preprocess(data_out)\n",
    "# Obtain labels\n",
    "y_labels = np.zeros((len(y_train),1))\n",
    "for i in range(len(y_train)):\n",
    "    y_labels[i] = np.argmax(y_train[i])\n",
    "y_labels = y_labels.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our classification model accurary we will use the our following defined metrics, based on the confusion matrix once the classification is performed. The first metric only considers misclassification error and the second one takes into account the fact that facies could be misclassified if they belong to a same group with similar geological characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(conf):\n",
    "    total_correct = 0.\n",
    "    nb_classes = conf.shape[0]\n",
    "    for i in np.arange(0,nb_classes):\n",
    "        total_correct += conf[i][i]\n",
    "    acc = total_correct/sum(sum(conf))\n",
    "    return acc\n",
    "\n",
    "adjacent_facies = np.array([[1], [0,2], [1], [4], [3,5], [4,6,7], [5,7], [5,6,8], [6,7]])\n",
    "facies_labels = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS','WS', 'D','PS', 'BS']\n",
    "\n",
    "def accuracy_adjacent(conf, adjacent_facies):\n",
    "    nb_classes = conf.shape[0]\n",
    "    total_correct = 0.\n",
    "    for i in np.arange(0,nb_classes):\n",
    "        total_correct += conf[i][i]\n",
    "        for j in adjacent_facies[i]:\n",
    "            total_correct += conf[i][j]\n",
    "    return total_correct / sum(sum(conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Deep Neural Network \n",
    "Our model will be composed by a Deep Neural Network of an input layer, two hidden layers and an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "input_dim = 77\n",
    "hidden_dim_1 = 200\n",
    "hidden_dim_2 = 50\n",
    "output_dim = 9 \n",
    "batch_size = 32\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dnn_model():\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=7, init='normal', activation='relu'))\n",
    "    model.add(Dense(50, input_dim=200, init='normal', activation='relu'))\n",
    "    model.add(Dense(9, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the set of parameters are fixed, the training stage of our model begins. We perform a Cross Validation routine to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cross Validation Results\n",
      "[ 0.55662651  0.52289157  0.50963855  0.43855422  0.46803378]\n",
      "Epoch 1/10\n",
      "0s - loss: 1.5783 - acc: 0.4268\n",
      "Epoch 2/10\n",
      "0s - loss: 1.1611 - acc: 0.5298\n",
      "Epoch 3/10\n",
      "0s - loss: 1.0903 - acc: 0.5642\n",
      "Epoch 4/10\n",
      "0s - loss: 1.0545 - acc: 0.5736\n",
      "Epoch 5/10\n",
      "0s - loss: 1.0272 - acc: 0.5809\n",
      "Epoch 6/10\n",
      "0s - loss: 1.0030 - acc: 0.5932\n",
      "Epoch 7/10\n",
      "0s - loss: 0.9870 - acc: 0.5951\n",
      "Epoch 8/10\n",
      "0s - loss: 0.9673 - acc: 0.5975\n",
      "Epoch 9/10\n",
      "0s - loss: 0.9459 - acc: 0.6117\n",
      "Epoch 10/10\n",
      "0s - loss: 0.9327 - acc: 0.6230\n",
      "\n",
      "Model Report\n",
      "-Accuracy: 0.624970\n",
      "-Adjacent Accuracy: 0.926006\n",
      "\n",
      "Confusion Matrix\n",
      "     Pred    SS  CSiS  FSiS  SiSh    MS    WS     D    PS    BS Total\n",
      "     True\n",
      "       SS   161    93    14                                       268\n",
      "     CSiS    66   599   272                 2           1         940\n",
      "     FSiS     4   168   596     2           3           7         780\n",
      "     SiSh     1     2     2   203     4    52     2     5         271\n",
      "       MS           6     8    41    12   168     5    56         296\n",
      "       WS     1           1    54    13   377    12   122     2   582\n",
      "        D                 1     9     2    12    82    33     2   141\n",
      "       PS           1    12    30     6   145    15   460    17   686\n",
      "       BS                       2          12    12    56   103   185\n",
      "\n",
      "Precision  0.69  0.69  0.66  0.60  0.32  0.49  0.64  0.62  0.83  0.62\n",
      "   Recall  0.60  0.64  0.76  0.75  0.04  0.65  0.58  0.67  0.56  0.62\n",
      "       F1  0.64  0.66  0.71  0.66  0.07  0.56  0.61  0.65  0.67  0.61\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "estimator = KerasClassifier(build_fn=dnn_model, nb_epoch=10, batch_size=1, verbose=0)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "results_dnn = cross_val_score(estimator, X_train, y_train, cv= skf.get_n_splits(X_train, y_train))\n",
    "print(' Cross Validation Results')\n",
    "print( results_dnn )\n",
    "\n",
    "# Load the model\n",
    "model_dnn = dnn_model()\n",
    "\n",
    "#Train model\n",
    "model_dnn.fit(X_train, y_train, nb_epoch=10, verbose=2)\n",
    "\n",
    "# Predict Values on Training set\n",
    "y_predicted = model_dnn.predict( X_train , batch_size=1, verbose=2)\n",
    "\n",
    "# Print Report\n",
    "\n",
    "# Format output [0 - 8 ]\n",
    "y_ = np.zeros((len(y_train),1))\n",
    "for i in range(len(y_train)):\n",
    "    y_[i] = np.argmax(y_train[i])\n",
    "\n",
    "y_predicted_ = np.zeros((len(y_predicted), 1))\n",
    "for i in range(len(y_predicted)):\n",
    "    y_predicted_[i] = np.argmax( y_predicted[i] )\n",
    "    \n",
    "# Confusion Matrix\n",
    "conf = confusion_matrix(y_, y_predicted_)\n",
    "\n",
    "# Print Results\n",
    "print (\"\\nModel Report\")\n",
    "print (\"-Accuracy: %.6f\" % ( accuracy(conf) ))\n",
    "print (\"-Adjacent Accuracy: %.6f\" % ( accuracy_adjacent(conf, adjacent_facies) ))\n",
    "print (\"\\nConfusion Matrix\")\n",
    "display_cm(conf, facies_labels, display_metrics=True, hide_zeros=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "---\n",
    "We obtain the predictions for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DNN model Prediction\n",
    "y_test = model_dnn.predict( X_test , batch_size=100, verbose=0)\n",
    "predictions_dnn = np.zeros((len(y_test),1))\n",
    "for i in range(len(y_test)):\n",
    "    predictions_dnn[i] = np.argmax(y_test[i]) + 1 \n",
    "predictions_dnn = predictions_dnn.astype(int)\n",
    "# Store results\n",
    "test_data = pd.read_csv('../validation_data_nofacies.csv')\n",
    "test_data['Facies'] = predictions_dnn\n",
    "test_data.to_csv('Prediction_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
