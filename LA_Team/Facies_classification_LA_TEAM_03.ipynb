{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facies classification using Machine Learning #\n",
    "## LA Team Submission 3 ## \n",
    "### _[Lukas Mosser](https://at.linkedin.com/in/lukas-mosser-9948b32b/en), [Alfredo De la Fuente](https://pe.linkedin.com/in/alfredodelafuenteb)_ ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this python notebook we explore several facies classification models such as Deep Neural Networks, Boosting Trees and  Bayesian, taking into account spatial dependencies to outperform the prediction model proposed in the [prediction facies from wel logs challenge](https://github.com/seg/2016-ml-contest). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Modeling\n",
    "----\n",
    "\n",
    "The dataset we will use comes from a class excercise from The University of Kansas on [Neural Networks and Fuzzy Systems](http://www.people.ku.edu/~gbohling/EECS833/).  This exercise is based on a consortium project to use machine learning techniques to create a reservoir model of the largest gas fields in North America, the Hugoton and Panoma Fields. For more info on the origin of the data, see [Bohling and Dubois (2003)](http://www.kgs.ku.edu/PRS/publication/2003/ofr2003-50.pdf) and [Dubois et al. (2007)](http://dx.doi.org/10.1016/j.cageo.2006.08.011). \n",
    "\n",
    "The dataset we will use is log data from nine wells that have been labeled with a facies type based on oberservation of core.  We will use this log data to train a classifier to predict facies types. \n",
    "\n",
    "This data is from the Council Grove gas reservoir in Southwest Kansas.  The Panoma Council Grove Field is predominantly a carbonate gas reservoir encompassing 2700 square miles in Southwestern Kansas.  This dataset is from nine wells (with 4149 examples), consisting of a set of seven predictor variables and a rock facies (class) for each example vector and validation (test) data (830 examples from two wells) having the same seven predictor variables in the feature vector.  Facies are based on examination of cores from nine wells taken vertically at half-foot intervals. Predictor variables include five from wireline log measurements and two geologic constraining variables that are derived from geologic knowledge. These are essentially continuous variables sampled at a half-foot sample rate. \n",
    "\n",
    "The seven predictor variables are:\n",
    "* Five wire line log curves include [gamma ray](http://petrowiki.org/Gamma_ray_logs) (GR), [resistivity logging](http://petrowiki.org/Resistivity_and_spontaneous_%28SP%29_logging) (ILD_log10),\n",
    "[photoelectric effect](http://www.glossary.oilfield.slb.com/en/Terms/p/photoelectric_effect.aspx) (PE), [neutron-density porosity difference and average neutron-density porosity](http://petrowiki.org/Neutron_porosity_logs) (DeltaPHI and PHIND). Note, some wells do not have PE.\n",
    "* Two geologic constraining variables: nonmarine-marine indicator (NM_M) and relative position (RELPOS)\n",
    "\n",
    "The nine discrete facies (classes of rocks) are: \n",
    "1. Nonmarine sandstone\n",
    "2. Nonmarine coarse siltstone \n",
    "3. Nonmarine fine siltstone \n",
    "4. Marine siltstone and shale \n",
    "5. Mudstone (limestone)\n",
    "6. Wackestone (limestone)\n",
    "7. Dolomite\n",
    "8. Packstone-grainstone (limestone)\n",
    "9. Phylloid-algal bafflestone (limestone)\n",
    "\n",
    "These facies aren't discrete, and gradually blend into one another. Some have neighboring facies that are rather close.  Mislabeling within these neighboring facies can be expected to occur.  The following table lists the facies, their abbreviated labels and their approximate neighbors.\n",
    "\n",
    "Facies |Label| Adjacent Facies\n",
    ":---: | :---: |:--:\n",
    "1 |SS| 2\n",
    "2 |CSiS| 1,3\n",
    "3 |FSiS| 2\n",
    "4 |SiSh| 5\n",
    "5 |MS| 4,6\n",
    "6 |WS| 5,7\n",
    "7 |D| 6,8\n",
    "8 |PS| 6,7,9\n",
    "9 |BS| 7,8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "---\n",
    "\n",
    "Let's import all the libraries that will be particularly needed for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): pandas in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): python-dateutil in /home/alfredo/anaconda2/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pytz>=2011k in /home/alfredo/anaconda2/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.7.0 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.5 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from python-dateutil->pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scikit-learn in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): keras in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): pyyaml in /home/alfredo/anaconda2/lib/python2.7/site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): theano in /home/alfredo/anaconda2/lib/python2.7/site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six in /home/alfredo/anaconda2/lib/python2.7/site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.7.1 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from theano->keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy>=0.11 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from theano->keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): xgboost in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from xgboost)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from xgboost)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scikit-learn in /home/alfredo/anaconda2/lib/python2.7/site-packages (from xgboost)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "pip install pandas\n",
    "pip install scikit-learn\n",
    "pip install keras\n",
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from classification_utilities import display_cm, display_adj_cm\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training and testing data to preprocess it for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Facies</th>\n",
       "      <th>Formation</th>\n",
       "      <th>Well Name</th>\n",
       "      <th>Depth</th>\n",
       "      <th>GR</th>\n",
       "      <th>ILD_log10</th>\n",
       "      <th>DeltaPHI</th>\n",
       "      <th>PHIND</th>\n",
       "      <th>PE</th>\n",
       "      <th>NM_M</th>\n",
       "      <th>RELPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2793.0</td>\n",
       "      <td>77.45</td>\n",
       "      <td>0.664</td>\n",
       "      <td>9.9</td>\n",
       "      <td>11.915</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2793.5</td>\n",
       "      <td>78.26</td>\n",
       "      <td>0.661</td>\n",
       "      <td>14.2</td>\n",
       "      <td>12.565</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2794.0</td>\n",
       "      <td>79.05</td>\n",
       "      <td>0.658</td>\n",
       "      <td>14.8</td>\n",
       "      <td>13.050</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2794.5</td>\n",
       "      <td>86.10</td>\n",
       "      <td>0.655</td>\n",
       "      <td>13.9</td>\n",
       "      <td>13.115</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2795.0</td>\n",
       "      <td>74.58</td>\n",
       "      <td>0.647</td>\n",
       "      <td>13.5</td>\n",
       "      <td>13.300</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2795.5</td>\n",
       "      <td>73.97</td>\n",
       "      <td>0.636</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.385</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2796.0</td>\n",
       "      <td>73.72</td>\n",
       "      <td>0.630</td>\n",
       "      <td>15.6</td>\n",
       "      <td>13.930</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2796.5</td>\n",
       "      <td>75.65</td>\n",
       "      <td>0.625</td>\n",
       "      <td>16.5</td>\n",
       "      <td>13.920</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2797.0</td>\n",
       "      <td>73.79</td>\n",
       "      <td>0.624</td>\n",
       "      <td>16.2</td>\n",
       "      <td>13.980</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2797.5</td>\n",
       "      <td>76.89</td>\n",
       "      <td>0.615</td>\n",
       "      <td>16.9</td>\n",
       "      <td>14.220</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Facies Formation  Well Name   Depth     GR  ILD_log10  DeltaPHI   PHIND  \\\n",
       "0       3     A1 SH  SHRIMPLIN  2793.0  77.45      0.664       9.9  11.915   \n",
       "1       3     A1 SH  SHRIMPLIN  2793.5  78.26      0.661      14.2  12.565   \n",
       "2       3     A1 SH  SHRIMPLIN  2794.0  79.05      0.658      14.8  13.050   \n",
       "3       3     A1 SH  SHRIMPLIN  2794.5  86.10      0.655      13.9  13.115   \n",
       "4       3     A1 SH  SHRIMPLIN  2795.0  74.58      0.647      13.5  13.300   \n",
       "5       3     A1 SH  SHRIMPLIN  2795.5  73.97      0.636      14.0  13.385   \n",
       "6       3     A1 SH  SHRIMPLIN  2796.0  73.72      0.630      15.6  13.930   \n",
       "7       3     A1 SH  SHRIMPLIN  2796.5  75.65      0.625      16.5  13.920   \n",
       "8       3     A1 SH  SHRIMPLIN  2797.0  73.79      0.624      16.2  13.980   \n",
       "9       3     A1 SH  SHRIMPLIN  2797.5  76.89      0.615      16.9  14.220   \n",
       "\n",
       "    PE  NM_M  RELPOS  \n",
       "0  4.6     1   1.000  \n",
       "1  4.1     1   0.979  \n",
       "2  3.6     1   0.957  \n",
       "3  3.5     1   0.936  \n",
       "4  3.4     1   0.915  \n",
       "5  3.6     1   0.894  \n",
       "6  3.7     1   0.872  \n",
       "7  3.5     1   0.830  \n",
       "8  3.4     1   0.809  \n",
       "9  3.5     1   0.787  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'train_test_data.csv'\n",
    "training_data = pd.read_csv(filename)\n",
    "training_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill the missing data values in the PE field with zero and proceed to normalize the data that will be fed into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set 'Well Name' and 'Formation' fields as categories\n",
    "training_data['Well Name'] = training_data['Well Name'].astype('category')\n",
    "training_data['Formation'] = training_data['Formation'].astype('category')\n",
    "\n",
    "# Fill missing values and normalize for 'PE' field\n",
    "training_data['PE'] = training_data['PE'].fillna(value=0)\n",
    "mean_pe = training_data['PE'].mean()\n",
    "std_pe = training_data['PE'].std()\n",
    "training_data['PE'] = (training_data['PE']-mean_pe)/std_pe\n",
    "\n",
    "# Normalize the rest of fields (GR, ILD_log10, DelthaPHI, PHIND,NM_M,RELPOS)\n",
    "correct_facies_labels = training_data['Facies'].values\n",
    "feature_vectors = training_data.drop(['Formation', 'Depth'], axis=1)\n",
    "well_labels = training_data[['Well Name', 'Facies']].values\n",
    "data_vectors = feature_vectors.drop(['Well Name', 'Facies'], axis=1).values\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(data_vectors)\n",
    "scaled_features = scaler.transform(data_vectors)\n",
    "data_out = np.hstack([well_labels, scaled_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start training stage, it is required to format the data by considering a sliding window over the depth component in order to classify a given set of features at some specific depth for each well in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data_out):\n",
    "    data = data_out\n",
    "    well_data = {}\n",
    "    well_names = list(set(data[:, 0]))\n",
    "    for name in well_names:\n",
    "        well_data[name] = [[], []]\n",
    "\n",
    "    for row in data:\n",
    "        well_data[row[0]][1].append(row[1])\n",
    "        well_data[row[0]][0].append(list(row[2::]))\n",
    "\n",
    "    # Sliding window\n",
    "    positive_lag = 5\n",
    "    negative_lag = 5\n",
    "\n",
    "    chunks = []\n",
    "    chunks_test = []\n",
    "    chunk_length = positive_lag+negative_lag+1 \n",
    "    chunks_facies = []\n",
    "    for name in well_names:\n",
    "        if name not in ['STUART', 'CRAWFORD']:\n",
    "            test_well_data = well_data[name]\n",
    "            log_values = np.array(test_well_data[0])\n",
    "            log_values_padded = np.lib.pad(log_values, (negative_lag,positive_lag), 'edge')[:, negative_lag:-positive_lag]\n",
    "            facies_values =  np.array(test_well_data[1])\n",
    "            for i in range(log_values.shape[0]):\n",
    "                chunks.append(log_values_padded[i:i+chunk_length, :])\n",
    "                chunks_facies.append(facies_values[i])\n",
    "        else:\n",
    "            test_well_data = well_data[name]\n",
    "            log_values = np.array(test_well_data[0])\n",
    "            log_values_padded = np.lib.pad(log_values, (negative_lag,positive_lag), 'edge')[:, negative_lag:-positive_lag]\n",
    "            facies_values =  np.array(test_well_data[1])\n",
    "            for i in range(log_values.shape[0]):\n",
    "                chunks_test.append(log_values_padded[i:i+chunk_length, :])\n",
    "    \n",
    "    chunks_facies = np.array(chunks_facies, dtype=np.int32)-1\n",
    "    X_ = np.array(chunks)\n",
    "    X = np.zeros((len(X_),len(X_[0][0]) * len(X_[0])))\n",
    "    for i in range(len(X_)):\n",
    "        X[i,:] = X_[i].flatten()\n",
    "        \n",
    "    X_test = np.array(chunks_test)\n",
    "    X_test_out = np.zeros((len(X_test),len(X_test[0][0]) * len(X_test[0])))\n",
    "    for i in range(len(X_test)):\n",
    "        X_test_out[i,:] = X_test[i].flatten()\n",
    "    y = np_utils.to_categorical(chunks_facies)\n",
    "    return X, y, X_test_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "---\n",
    "\n",
    "We will experiment with an ensemble of classification models to outperform the accuracy at predicting facies. As input we will consider a set of features extracted by padding a depth interval segment, that way we take into account spatial dependencies. As output we obtain a vector filled with values ranging from [0-8] that indicate the presence of each facie with respect to depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "np.random.seed(512) \n",
    "# Load data\n",
    "X_train, y_train, X_test = preprocess(data_out)\n",
    "# Obtain labels\n",
    "y_labels = np.zeros((len(y_train),1))\n",
    "for i in range(len(y_train)):\n",
    "    y_labels[i] = np.argmax(y_train[i])\n",
    "y_labels = y_labels.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our classification model accurary we will use the our following defined metrics, based on the confusion matrix once the classification is performed. The first metric only considers misclassification error and the second one takes into account the fact that facies could be misclassified if they belong to a same group with similar geological characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(conf):\n",
    "    total_correct = 0.\n",
    "    nb_classes = conf.shape[0]\n",
    "    for i in np.arange(0,nb_classes):\n",
    "        total_correct += conf[i][i]\n",
    "    acc = total_correct/sum(sum(conf))\n",
    "    return acc\n",
    "\n",
    "adjacent_facies = np.array([[1], [0,2], [1], [4], [3,5], [4,6,7], [5,7], [5,6,8], [6,7]])\n",
    "facies_labels = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS','WS', 'D','PS', 'BS']\n",
    "\n",
    "def accuracy_adjacent(conf, adjacent_facies):\n",
    "    nb_classes = conf.shape[0]\n",
    "    total_correct = 0.\n",
    "    for i in np.arange(0,nb_classes):\n",
    "        total_correct += conf[i][i]\n",
    "        for j in adjacent_facies[i]:\n",
    "            total_correct += conf[i][j]\n",
    "    return total_correct / sum(sum(conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Deep Neural Network \n",
    "Our model will be composed by a Deep Neural Network of an input layer, two hidden layers and an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "input_dim = 77\n",
    "hidden_dim_1 = 128\n",
    "hidden_dim_2 = 32\n",
    "output_dim = 9 \n",
    "batch_size = 32\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dnn_model():\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=77, init='normal', activation='relu'))\n",
    "    model.add(Dense(32, input_dim=128, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(9, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the set of parameters are fixed, the training stage of our model begins. We perform a Cross Validation routine to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cross Validation Results\n",
      "[ 0.61686747  0.63253012  0.62289157  0.65662651  0.61037395]\n",
      "\n",
      "Model Report\n",
      "-Accuracy: 0.683056\n",
      "-Adjacent Accuracy: 0.946493\n",
      "\n",
      "Confusion Matrix\n",
      "     Pred    SS  CSiS  FSiS  SiSh    MS    WS     D    PS    BS Total\n",
      "     True\n",
      "       SS   180    81     7                                       268\n",
      "     CSiS    21   661   256           1                 1         940\n",
      "     FSiS     1   161   612           1     1           4         780\n",
      "     SiSh           2     3   215     5    35     5     6         271\n",
      "       MS           6     6    23    67   142    10    42         296\n",
      "       WS                 1    40    61   313    11   152     4   582\n",
      "        D                 1           2     6   115    17         141\n",
      "       PS                 5     6    19    88     7   525    36   686\n",
      "       BS                       5           8     5    21   146   185\n",
      "\n",
      "Precision  0.89  0.73  0.69  0.74  0.43  0.53  0.75  0.68  0.78  0.68\n",
      "   Recall  0.67  0.70  0.78  0.79  0.23  0.54  0.82  0.77  0.79  0.68\n",
      "       F1  0.77  0.71  0.73  0.77  0.30  0.53  0.78  0.72  0.79  0.68\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "estimator = KerasClassifier(build_fn=dnn_model, nb_epoch=10, batch_size=50, verbose=0)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "results_dnn = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(' Cross Validation Results')\n",
    "print( results_dnn )\n",
    "\n",
    "# Load the model\n",
    "model_dnn = dnn_model()\n",
    "\n",
    "#Train model\n",
    "model_dnn.fit(X_train, y_train, nb_epoch=10, verbose=0, shuffle = True)\n",
    "\n",
    "# Predict Values on Training set\n",
    "y_predicted = model_dnn.predict( X_train , batch_size=32, verbose=0)\n",
    "\n",
    "# Print Report\n",
    "\n",
    "# Format output [0 - 8 ]\n",
    "y_ = np.zeros((len(y_train),1))\n",
    "for i in range(len(y_train)):\n",
    "    y_[i] = np.argmax(y_train[i])\n",
    "\n",
    "y_predicted_ = np.zeros((len(y_predicted), 1))\n",
    "for i in range(len(y_predicted)):\n",
    "    y_predicted_[i] = np.argmax( y_predicted[i] )\n",
    "    \n",
    "# Confusion Matrix\n",
    "conf = confusion_matrix(y_, y_predicted_)\n",
    "\n",
    "# Print Results\n",
    "print (\"\\nModel Report\")\n",
    "print (\"-Accuracy: %.6f\" % ( accuracy(conf) ))\n",
    "print (\"-Adjacent Accuracy: %.6f\" % ( accuracy_adjacent(conf, adjacent_facies) ))\n",
    "print (\"\\nConfusion Matrix\")\n",
    "display_cm(conf, facies_labels, display_metrics=True, hide_zeros=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Deep Neural Network 2 \n",
    "Our model will be composed by a Deep Neural Network of an input layer, three hidden layers and an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "input_dim = 77\n",
    "hidden_dim_1 = 128\n",
    "hidden_dim_2 = 64\n",
    "output_dim = 9 \n",
    "batch_size = 32\n",
    "nb_epoch = 10\n",
    "\n",
    "def dnn_model_2 ():\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=77, init='normal', activation='relu'))\n",
    "    model.add(Dense(128, input_dim=128, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, input_dim=128, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(9, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cross Validation Results\n",
      "[ 0.7686747   0.79156626  0.76385541  0.76987952  0.77442702]\n",
      "\n",
      "Model Report\n",
      "-Accuracy: 0.955893\n",
      "-Adjacent Accuracy: 0.995662\n",
      "\n",
      "Confusion Matrix\n",
      "     Pred    SS  CSiS  FSiS  SiSh    MS    WS     D    PS    BS Total\n",
      "     True\n",
      "       SS   265     3                                             268\n",
      "     CSiS    22   902    16                                       940\n",
      "     FSiS          34   746                                       780\n",
      "     SiSh                     259     4     6     1     1         271\n",
      "       MS                       1   264    29           2         296\n",
      "       WS                            14   561           6     1   582\n",
      "        D                             1     4   134     2         141\n",
      "       PS                       1     5    29         651         686\n",
      "       BS                                               1   184   185\n",
      "\n",
      "Precision  0.92  0.96  0.98  0.99  0.92  0.89  0.99  0.98  0.99  0.96\n",
      "   Recall  0.99  0.96  0.96  0.96  0.89  0.96  0.95  0.95  0.99  0.96\n",
      "       F1  0.95  0.96  0.97  0.97  0.90  0.93  0.97  0.97  0.99  0.96\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "estimator_2 = KerasClassifier(build_fn=dnn_model_2, nb_epoch=50, batch_size=50, verbose=0)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "results_dnn_2 = cross_val_score(estimator_2, X_train, y_train, cv=kfold)\n",
    "print(' Cross Validation Results')\n",
    "print( results_dnn_2 )\n",
    "\n",
    "# Load the model\n",
    "model_dnn_2 = dnn_model_2()\n",
    "#Train model\n",
    "model_dnn_2.fit(X_train, y_train, nb_epoch=50, verbose=0, shuffle = True)\n",
    "\n",
    "\n",
    "# Predict Values on Training set\n",
    "y_predicted = model_dnn_2.predict( X_train , batch_size=32, verbose=0)\n",
    "\n",
    "# Print Report\n",
    "\n",
    "# Format output [0 - 8 ]\n",
    "y_ = np.zeros((len(y_train),1))\n",
    "for i in range(len(y_train)):\n",
    "    y_[i] = np.argmax(y_train[i])\n",
    "\n",
    "y_predicted_ = np.zeros((len(y_predicted), 1))\n",
    "for i in range(len(y_predicted)):\n",
    "    y_predicted_[i] = np.argmax( y_predicted[i] )\n",
    "    \n",
    "# Confusion Matrix\n",
    "conf = confusion_matrix(y_, y_predicted_)\n",
    "\n",
    "# Print Results\n",
    "print (\"\\nModel Report\")\n",
    "print (\"-Accuracy: %.6f\" % ( accuracy(conf) ))\n",
    "print (\"-Adjacent Accuracy: %.6f\" % ( accuracy_adjacent(conf, adjacent_facies) ))\n",
    "print (\"\\nConfusion Matrix\")\n",
    "display_cm(conf, facies_labels, display_metrics=True, hide_zeros=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - XGBoost\n",
    "\n",
    "\n",
    "XGBoost (or Extreme Gradient Boosting) is a sophisticated algorithm that corresponds to an advanced implementation of gradient boosting algorithm. Despite it's complexity, it is fairly easy to use and very powerful to deal with many different types of irregularities in data. It has been no surprise then, that is has been extensively applied in many machine learning competitions to obtain very promising results.\n",
    "\n",
    "\n",
    "Although XGBoost is a very straightforward algorithm to implement, it's difficulty arises in dealing with a big number of hyperparameters. For that reason, we need to develop routines to tune this parameters to optimize the algorithm performance on the data prediction.\n",
    "\n",
    "We will use a Cross-Validation approach to improve our model by tuning parameters at each step. There are three types of parameters to consider:\n",
    "\n",
    "- General Parameters: Guide the overall functioning\n",
    "- Booster Parameters: Guide the individual booster (tree/regression) at each step\n",
    "- Learning Task Parameters: Guide the optimization performed\n",
    "\n",
    "We must realize that our training data is quite reduced, therefore to evaluate our model generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation Training Report Summary\n",
      "    test-merror-mean  test-merror-std  train-merror-mean  train-merror-std\n",
      "95          0.333823         0.019476           0.196133          0.006285\n",
      "96          0.334063         0.019865           0.196012          0.006063\n",
      "97          0.333580         0.019670           0.196193          0.005843\n",
      "98          0.333098         0.019022           0.195952          0.005570\n",
      "99          0.333098         0.019022           0.195831          0.006023\n",
      " Cross Validation Results\n",
      "[ 0.67228916  0.70963855  0.64337349  0.68192771  0.62967431]\n",
      "\n",
      "Confusion Matrix\n",
      "     Pred    SS  CSiS  FSiS  SiSh    MS    WS     D    PS    BS Total\n",
      "     True\n",
      "       SS   173    86     9                                       268\n",
      "     CSiS    14   829    91     1           2           3         940\n",
      "     FSiS         155   617     2           3           3         780\n",
      "     SiSh           1     4   216     1    42     1     6         271\n",
      "       MS           6     6     9   167    74     4    30         296\n",
      "       WS                 2    21     6   478     4    71         582\n",
      "        D                 1     3          10   106    21         141\n",
      "       PS                 7     3     1    95     5   575         686\n",
      "       BS                                   8          23   154   185\n",
      "\n",
      "Precision  0.93  0.77  0.84  0.85  0.95  0.67  0.88  0.79  1.00  0.81\n",
      "   Recall  0.65  0.88  0.79  0.80  0.56  0.82  0.75  0.84  0.83  0.80\n",
      "       F1  0.76  0.82  0.81  0.82  0.71  0.74  0.81  0.81  0.91  0.80\n",
      "\n",
      "Model Report\n",
      "-Accuracy: 0.798988\n",
      "-Adjacent Accuracy: 0.959267\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "rounds = 100\n",
    "cv_folds = 5 \n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_labels)\n",
    "\n",
    "model_xgb = xgb.XGBClassifier( learning_rate =0.001, n_estimators=500, max_depth=6,\n",
    "                               min_child_weight=1, gamma=0, subsample=0.8, reg_alpha = 1,\n",
    "                               colsample_bytree=0.8, objective='multi:softmax',\n",
    "                               nthread=4, scale_pos_weight=1, seed=512)\n",
    "\n",
    "xgb_param = model_xgb.get_xgb_params()\n",
    "xgb_param['num_class'] = 9\n",
    "\n",
    "# Perform cross-validation\n",
    "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=xgb_param['n_estimators'], \n",
    "                  stratified = True, nfold=cv_folds, metrics='merror', early_stopping_rounds=rounds)\n",
    "\n",
    "print (\"\\nCross Validation Training Report Summary\")\n",
    "print ( cvresult.tail() )\n",
    "\n",
    "\n",
    "model_xgb.set_params(n_estimators=cvresult.shape[0])\n",
    "\n",
    "\n",
    "# Cross Validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "results_xgb = cross_val_score( model_xgb , X_train, y_labels, cv=kfold)\n",
    "print(' Cross Validation Results')\n",
    "print( results_xgb )\n",
    "\n",
    "\n",
    "#Fit the algorithm on the data\n",
    "model_xgb.fit(X_train, y_labels ,eval_metric='merror')\n",
    "\n",
    "#Predict training set:\n",
    "predictions = model_xgb.predict(X_train)\n",
    "        \n",
    "#Print model report\n",
    "\n",
    "# Confusion Matrix\n",
    "conf = confusion_matrix(y_labels, predictions )\n",
    "\n",
    "# Confusion Matrix\n",
    "print (\"\\nConfusion Matrix\")\n",
    "display_cm(conf, facies_labels, display_metrics=True, hide_zeros=True)\n",
    "\n",
    "# Print Results\n",
    "print (\"\\nModel Report\")\n",
    "print (\"-Accuracy: %.6f\" % ( accuracy(conf) ))\n",
    "print (\"-Adjacent Accuracy: %.6f\" % ( accuracy_adjacent(conf, adjacent_facies) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "---\n",
    "We obtain the predictions for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DNN model Prediction\n",
    "y_test = model_dnn.predict( X_test , batch_size=32, verbose=0)\n",
    "predictions_dnn = np.zeros((len(y_test),1))\n",
    "for i in range(len(y_test)):\n",
    "    predictions_dnn[i] = np.argmax(y_test[i]) + 1 \n",
    "predictions_dnn = predictions_dnn.astype(int)\n",
    "# Store results\n",
    "test_data = pd.read_csv('../validation_data_nofacies.csv')\n",
    "test_data['Facies'] = predictions_dnn\n",
    "test_data.to_csv('Prediction_DNN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DNN2 model Prediction\n",
    "y_test_2 = model_dnn_2.predict( X_test , batch_size=32, verbose=0)\n",
    "predictions_dnn_2 = np.zeros((len(y_test),1))\n",
    "for i in range(len(y_test)):\n",
    "    predictions_dnn_2[i] = np.argmax(y_test_2[i]) + 1 \n",
    "predictions_dnn_2 = predictions_dnn_2.astype(int)\n",
    "# Store results\n",
    "test_data = pd.read_csv('../validation_data_nofacies.csv')\n",
    "test_data['Facies'] = predictions_dnn_2\n",
    "test_data.to_csv('Prediction_DNN2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# XGBoost model Prediction\n",
    "predictions_xgb = model_xgb.predict(X_test) + 1\n",
    "# Store results\n",
    "test_data = pd.read_csv('../validation_data_nofacies.csv')\n",
    "test_data['Facies'] = predictions_xgb\n",
    "test_data.to_csv('Prediction_XGB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embedded model\n",
    "\n",
    "# Weights\n",
    "coef_dnn = results_dnn.mean()\n",
    "coef_xgb = results_xgb.mean()\n",
    "coef_dnn_2 = results_dnn_2.mean()\n",
    "\n",
    "# Weighted Average\n",
    "predictions_final = np.zeros((len(X_test),1))\n",
    "for i in range( len(X_test ) ):\n",
    "    predictions_final[i] = ( int ) ( round ( ( coef_dnn * predictions_dnn[i] +\n",
    "                                       coef_xgb * predictions_xgb[i] +\n",
    "                                        coef_dnn_2 * predictions_dnn_2[i] ) / \n",
    "                                     ( coef_dnn + coef_xgb + coef_dnn_2 ) )  )\n",
    "    \n",
    "predictions_final = predictions_final.astype(int)\n",
    "    \n",
    "# Store results\n",
    "test_data = pd.read_csv('../validation_data_nofacies.csv')\n",
    "test_data['Facies'] = predictions_final\n",
    "test_data.to_csv('Prediction_FINAL.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
