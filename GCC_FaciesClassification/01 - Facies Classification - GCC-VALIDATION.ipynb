{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facies Classification using TPOT\n",
    "### George Crowther - https://www.linkedin.com/in/george-crowther-9669a931?trk=hp-identity-name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've had a play with some of the data here and used something of a brute force approach, by creating a large number of additional features and then using the TPOT library to train a model and refine the model parameters. I will be interested to see whether this has over-fitted, as the selected Extra Trees Classifier can do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Initial Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"69e312c0-9f03-48c3-84df-8d0f90586698\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#69e312c0-9f03-48c3-84df-8d0f90586698\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"69e312c0-9f03-48c3-84df-8d0f90586698\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '69e312c0-9f03-48c3-84df-8d0f90586698' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#69e312c0-9f03-48c3-84df-8d0f90586698\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#69e312c0-9f03-48c3-84df-8d0f90586698\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial imports for reading data and first observations\n",
    "import pandas as pd\n",
    "import bokeh.plotting as bk\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "bk.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input file paths\n",
    "train_path = r'../training_data.csv'\n",
    "test_path = r'../validation_data_nofacies.csv'\n",
    "\n",
    "# Read training data to dataframe\n",
    "train = pd.read_csv(train_path)\n",
    "\n",
    "# TPOT library requires that the target class is renamed to 'class'\n",
    "train.rename(columns={'Facies': 'class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formations = {}\n",
    "for i, value in enumerate(train['Formation'].unique()):\n",
    "    formations[value] = i\n",
    "    train.loc[train['Formation'] == value, 'Formation'] = i\n",
    "\n",
    "wells = {}\n",
    "for i, value in enumerate(train['Well Name'].unique()):\n",
    "    wells[value] = i\n",
    "    train.loc[train['Well Name'] == value, 'Well Name'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "facies_labels = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS',\n",
    "                 'WS', 'D','PS', 'BS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature construction and data clean-up.\n",
    "    1. Z-score normalisation of data.\n",
    "    2. Group each of the measurement parameters into quartiles. Most of the classification methods find data like this easier to work with.\n",
    "    3. Create a series of 'adjacent' parameters by looking for the above and below depth sample for each well. Create a series of features associated with the above and below parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_columns = train.columns[1:]\n",
    "std_scaler = preprocessing.StandardScaler().fit(train[train_columns])\n",
    "\n",
    "train_std = std_scaler.transform(train[train_columns])\n",
    "\n",
    "train_std_frame = train\n",
    "for i, column in enumerate(train_columns):\n",
    "    train_std_frame.loc[:, column] = train_std[:, i]\n",
    "\n",
    "train = train_std_frame\n",
    "master_columns = train.columns[4:]\n",
    "\n",
    "def in_range(row, vmin, vmax, variable):\n",
    "    if vmin <= row[variable] < vmax:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "for i, column in train[master_columns].iteritems():\n",
    "    ds = np.linspace(0, 1.0, 5)\n",
    "    quantiles = [column.quantile(n) for n in ds]\n",
    "    \n",
    "    for j in range(len(quantiles) - 1):\n",
    "        train[i + '_{0}'.format(j)] = train.apply(lambda row: in_range(row, ds[j], ds[j + 1], i), axis = 1)\n",
    "\n",
    "master_columns = train.columns[4:]\n",
    "\n",
    "above = []\n",
    "below = []\n",
    "for i, group in train.groupby('Well Name'):\n",
    "    \n",
    "    df = group.sort_values('Depth')\n",
    "    u = df.shift(-1).fillna(method = 'ffill')\n",
    "    b = df.shift(1).fillna(method = 'bfill')\n",
    "    \n",
    "    above.append(u[master_columns])\n",
    "    below.append(b[master_columns])\n",
    "    \n",
    "above_frame = pd.concat(above)\n",
    "above_frame.columns = ['above_'+ column for column in above_frame.columns]\n",
    "below_frame = pd.concat(below)\n",
    "below_frame.columns = ['below_'+ column for column in below_frame.columns]\n",
    "\n",
    "frame = pd.concat((train, above_frame, below_frame), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vector = ['class']\n",
    "train_columns = frame.columns[4:]\n",
    "\n",
    "# train_f, test_f = train_test_split(frame, test_size = 0.1, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPOT uses a genetic algorithm to tune model parameters for the most effective fit. This can take quite a while to process if you want to re-run this part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tpot = TPOTClassifier(verbosity=2, generations=5, max_eval_time_mins=30)\n",
    "# tpot.fit(train_f[train_columns], train_f['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tpot.score(test_f[train_columns], test_f['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tpot.export('contest_export.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\r",
      "\r\n",
      "\r",
      "\r\n",
      "from sklearn.ensemble import ExtraTreesClassifier, VotingClassifier\r",
      "\r\n",
      "from sklearn.feature_selection import VarianceThreshold\r",
      "\r\n",
      "from sklearn.model_selection import train_test_split\r",
      "\r\n",
      "from sklearn.pipeline import make_pipeline, make_union\r",
      "\r\n",
      "from sklearn.preprocessing import FunctionTransformer\r",
      "\r\n",
      "\r",
      "\r\n",
      "# NOTE: Make sure that the class is labeled 'class' in the data file\r",
      "\r\n",
      "tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)\r",
      "\r\n",
      "features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)\r",
      "\r\n",
      "training_features, testing_features, training_classes, testing_classes = \\\r",
      "\r\n",
      "    train_test_split(features, tpot_data['class'], random_state=42)\r",
      "\r\n",
      "\r",
      "\r\n",
      "exported_pipeline = make_pipeline(\r",
      "\r\n",
      "    VarianceThreshold(threshold=0.37),\r",
      "\r\n",
      "    ExtraTreesClassifier(criterion=\"entropy\", max_features=0.71, n_estimators=500)\r",
      "\r\n",
      ")\r",
      "\r\n",
      "\r",
      "\r\n",
      "exported_pipeline.fit(training_features, training_classes)\r",
      "\r\n",
      "results = exported_pipeline.predict(testing_features)\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat contest_export.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "clf = make_pipeline(\n",
    "    VarianceThreshold(threshold=0.37),\n",
    "    ExtraTreesClassifier(criterion=\"entropy\", max_features=0.71, n_estimators=500, random_state=49)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('variancethreshold', VarianceThreshold(threshold=0.37)), ('extratreesclassifier', ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
       "           max_depth=None, max_features=0.71, max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=500, n_jobs=1, oob_score=False, random_state=49,\n",
       "           verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(frame[train_columns], frame['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Workflow for Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to generate results from output model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_path = r'../validation_data_nofacies.csv'\n",
    "\n",
    "# Read training data to dataframe\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "# TPOT library requires that the target class is renamed to 'class'\n",
    "test.rename(columns={'Facies': 'class'}, inplace=True)\n",
    "\n",
    "test_columns = test.columns\n",
    "\n",
    "formations = {}\n",
    "for i, value in enumerate(test['Formation'].unique()):\n",
    "    formations[value] = i\n",
    "    test.loc[test['Formation'] == value, 'Formation'] = i\n",
    "\n",
    "wells = {}\n",
    "for i, value in enumerate(test['Well Name'].unique()):\n",
    "    wells[value] = i\n",
    "    test.loc[test['Well Name'] == value, 'Well Name'] = i\n",
    "\n",
    "std_scaler = preprocessing.StandardScaler().fit(test[test_columns])\n",
    "test_std = std_scaler.transform(test[test_columns])\n",
    "\n",
    "test_std_frame = test\n",
    "for i, column in enumerate(test_columns):\n",
    "    test_std_frame.loc[:, column] = test_std[:, i]\n",
    "\n",
    "test = test_std_frame\n",
    "master_columns = test.columns[3:]\n",
    "\n",
    "def in_range(row, vmin, vmax, variable):\n",
    "    \n",
    "    if vmin <= row[variable] < vmax:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "for i, column in test[master_columns].iteritems():\n",
    "    ds = np.linspace(0, 1.0, 5)\n",
    "    quantiles = [column.quantile(n) for n in ds]\n",
    "    \n",
    "    for j in range(len(quantiles) - 1):\n",
    "        test[i + '_{0}'.format(j)] = test.apply(lambda row: in_range(row, ds[j], ds[j + 1], i), axis = 1)\n",
    "\n",
    "master_columns = test.columns[3:]\n",
    "\n",
    "above = []\n",
    "below = []\n",
    "for i, group in test.groupby('Well Name'):\n",
    "    \n",
    "    df = group.sort_values('Depth')\n",
    "    u = df.shift(-1).fillna(method = 'ffill')\n",
    "    b = df.shift(1).fillna(method = 'bfill')\n",
    "    \n",
    "    above.append(u[master_columns])\n",
    "    below.append(b[master_columns])\n",
    "    \n",
    "above_frame = pd.concat(above)\n",
    "above_frame.columns = ['above_'+ column for column in above_frame.columns]\n",
    "below_frame = pd.concat(below)\n",
    "below_frame.columns = ['below_'+ column for column in below_frame.columns]\n",
    "\n",
    "frame = pd.concat((test, above_frame, below_frame), axis = 1)\n",
    "\n",
    "test_columns = frame.columns[3:]\n",
    "\n",
    "result = clf.predict(frame[test_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2,\n",
       "       2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 8, 8, 8,\n",
       "       8, 6, 6, 6, 6, 6, 6, 6, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 4, 4,\n",
       "       6, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, 6,\n",
       "       6, 6, 8, 8, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 4, 4, 5, 7, 4, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 3, 2,\n",
       "       3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 9, 9, 8, 9, 9, 9, 9, 8, 6, 6, 6, 6, 8, 2, 2, 3, 3,\n",
       "       3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       3, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6, 8, 3, 3, 3, 3, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 8, 8, 8, 6, 6, 6, 6, 6, 9, 9, 9,\n",
       "       9, 9, 9, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 5, 6, 5, 6, 6, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 4,\n",
       "       4, 4, 4, 4, 8, 4, 4, 4, 4, 6, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 6, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 6, 8, 8, 8, 6, 6, 4, 4, 4, 4, 6, 6, 8, 8, 8, 8, 8, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 5, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 6, 6, 8,\n",
       "       3, 3, 8, 8, 8, 6, 6, 8, 8, 8, 3, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 6, 6, 6, 8, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 9, 9, 9, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 3,\n",
       "       3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6, 8, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 5, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6, 6, 6,\n",
       "       6, 6, 8, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 5, 8, 8,\n",
       "       6, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_frame = pd.read_csv(test_path)\n",
    "output_frame['Facies'] = result\n",
    "output_frame.to_csv('Well Facies Prediction - Test Data Set__MATT3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:aiplay3]",
   "language": "python",
   "name": "conda-env-aiplay3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
