{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 - Facies Classification Using RandomForestClassifier.\n",
    "\n",
    "## Chris Esprey - https://www.linkedin.com/in/christopher-esprey-beng-8aab1078?trk=nav_responsive_tab_profile\n",
    "\n",
    "I have generated two main feature types, namely:\n",
    "\n",
    "- The absolute difference between each feature for all feature rows.\n",
    "- The difference between each sample and the mean and standard deviation of each facies.\n",
    "\n",
    "I then threw this at a RandomForestClassifier. \n",
    "\n",
    "Possible future improvements:\n",
    "- Perform Univariate feature selection to hone in on the best features\n",
    "- Try out other classifiers e.g. gradient boost, SVM etc. \n",
    "- Use an ensemble of algorithms for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris.esprey\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from classification_utilities import display_cm, display_adj_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'training_data.csv'\n",
    "training_data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 - Feature Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create a difference vector for each feature e.g. x1-x2, x1-x3... x2-x3...\n",
    "\n",
    "# order features in depth.\n",
    "\n",
    "feature_vectors = training_data.drop(['Formation', 'Well Name','Facies'], axis=1)\n",
    "feature_vectors = feature_vectors[['Depth','GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE']]\n",
    "\n",
    "def difference_vector(feature_vectors):\n",
    "    length = len(feature_vectors['Depth'])\n",
    "    df_temp = np.zeros((25, length))\n",
    "                          \n",
    "    for i in range(0,int(len(feature_vectors['Depth']))):\n",
    "                       \n",
    "        vector_i = feature_vectors.iloc[i,:]\n",
    "        vector_i = vector_i[['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE']]\n",
    "        for j, value_j in enumerate(vector_i):\n",
    "            for k, value_k in enumerate(vector_i): \n",
    "                differ_j_k = value_j - value_k          \n",
    "                df_temp[5*j+k, i] = np.abs(differ_j_k)\n",
    "                \n",
    "    return df_temp\n",
    "\n",
    "def diff_vec2frame(feature_vectors, df_temp):\n",
    "    \n",
    "    heads = feature_vectors.columns[1::]\n",
    "    for i in range(0,5):\n",
    "        string_i = heads[i]\n",
    "        for j in range(0,5):\n",
    "            string_j = heads[j]\n",
    "            col_head = 'diff'+string_i+string_j\n",
    "            \n",
    "            df = pd.Series(df_temp[5*i+j, :])\n",
    "            feature_vectors[col_head] = df\n",
    "    return feature_vectors\n",
    "            \n",
    "df_diff = difference_vector(feature_vectors)    \n",
    "feature_vectors = diff_vec2frame(feature_vectors, df_diff)\n",
    "\n",
    "# Drop duplicated columns and column of zeros\n",
    "feature_vectors = feature_vectors.T.drop_duplicates().T   \n",
    "feature_vectors.drop('diffGRGR', axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add Facies column back into features vector\n",
    "\n",
    "feature_vectors['Facies'] = training_data['Facies']\n",
    "\n",
    "# # group by facies, take statistics of each facies e.g. mean, std. Take sample difference of each row with \n",
    "\n",
    "def facies_stats(feature_vectors):\n",
    "    facies_labels = np.sort(feature_vectors['Facies'].unique())\n",
    "    frame_mean = pd.DataFrame()\n",
    "    frame_std = pd.DataFrame()\n",
    "    for i, value in enumerate(facies_labels):\n",
    "        facies_subframe = feature_vectors[feature_vectors['Facies']==value]\n",
    "        subframe_mean = facies_subframe.mean()\n",
    "        subframe_std = facies_subframe.std()\n",
    "        \n",
    "        frame_mean[str(value)] = subframe_mean\n",
    "        frame_std[str(value)] = subframe_std\n",
    "    \n",
    "    return frame_mean.T, frame_std.T\n",
    "\n",
    "def feature_stat_diff(feature_vectors, frame_mean, frame_std):\n",
    "    \n",
    "    feature_vec_origin = feature_vectors[['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE']]\n",
    "    \n",
    "    for i, column in enumerate(feature_vec_origin):\n",
    "        \n",
    "        feature_column = feature_vec_origin[column]\n",
    "        stat_column_mean = frame_mean[column]\n",
    "        stat_column_std = frame_std[column]\n",
    "        \n",
    "        for j in range(0,9):\n",
    "            \n",
    "            stat_column_mean_facie = stat_column_mean[j]\n",
    "            stat_column_std_facie = stat_column_std[j]\n",
    "            \n",
    "            feature_vectors[column + '_mean_diff_facies' + str(j)] = feature_column-stat_column_mean_facie\n",
    "            feature_vectors[column + '_std_diff_facies' + str(j)] = feature_column-stat_column_std_facie\n",
    "    return feature_vectors\n",
    "             \n",
    "frame_mean, frame_std = facies_stats(feature_vectors)  \n",
    "feature_vectors = feature_stat_diff(feature_vectors, frame_mean, frame_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 - Generate plots of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A = feature_vectors.sort_values(by='Facies')\n",
    "# A.reset_index(drop=True).plot(subplots=True, style='b', figsize = [12, 400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 - Train model using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = feature_vectors\n",
    "predictors = feature_vectors.columns\n",
    "predictors = list(predictors.drop('Facies'))\n",
    "correct_facies_labels = df['Facies'].values\n",
    "# Scale features\n",
    "df = df[predictors]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(df)\n",
    "scaled_features = scaler.transform(df)\n",
    "\n",
    "# Train test split:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features,  correct_facies_labels, test_size=0.2, random_state=0)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=200, min_samples_split=8, min_samples_leaf=3, max_features= None)\n",
    "alg.fit(X_train, y_train)\n",
    "\n",
    "predicted_random_forest = alg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pred    SS  CSiS  FSiS  SiSh    MS    WS     D    PS    BS Total\n",
      "     True\n",
      "       SS    31     7     3                 1                      42\n",
      "     CSiS     6   125    21                 2           1         155\n",
      "     FSiS     1    24    88           1     2     1     2         119\n",
      "     SiSh           5     2    19     3     4           3          36\n",
      "       MS           4     2          20     4           3          33\n",
      "       WS           5           3     5    76     1    14         104\n",
      "        D           1                       1     9     4     1    16\n",
      "       PS           5     1           2    16     2    76     4   106\n",
      "       BS                             1     2     2     5    26    36\n",
      "\n",
      "Precision  0.82  0.71  0.75  0.86  0.62  0.70  0.60  0.70  0.84  0.73\n",
      "   Recall  0.74  0.81  0.74  0.53  0.61  0.73  0.56  0.72  0.72  0.73\n",
      "       F1  0.78  0.76  0.75  0.66  0.62  0.72  0.58  0.71  0.78  0.73\n",
      "0.726429675425\n",
      "0.910355486862\n"
     ]
    }
   ],
   "source": [
    "facies_labels = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS',\n",
    "                 'WS', 'D','PS', 'BS']\n",
    "result = predicted_random_forest\n",
    "conf = confusion_matrix(y_test, result)\n",
    "display_cm(conf, facies_labels, hide_zeros=True, display_metrics = True)\n",
    "\n",
    "def accuracy(conf):\n",
    "    total_correct = 0.\n",
    "    nb_classes = conf.shape[0]\n",
    "    for i in np.arange(0,nb_classes):\n",
    "        total_correct += conf[i][i]\n",
    "    acc = total_correct/sum(sum(conf))\n",
    "    return acc\n",
    "\n",
    "print(accuracy(conf))\n",
    "\n",
    "adjacent_facies = np.array([[1], [0,2], [1], [4], [3,5], [4,6,7], [5,7], [5,6,8], [6,7]])\n",
    "\n",
    "def accuracy_adjacent(conf, adjacent_facies):\n",
    "    nb_classes = conf.shape[0]\n",
    "    total_correct = 0.\n",
    "    for i in np.arange(0,nb_classes):\n",
    "        total_correct += conf[i][i]\n",
    "        for j in adjacent_facies[i]:\n",
    "            total_correct += conf[i][j]\n",
    "    return total_correct / sum(sum(conf))\n",
    "\n",
    "print(accuracy_adjacent(conf, adjacent_facies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 - Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in Test data\n",
    "\n",
    "filename = 'validation_data_nofacies.csv'\n",
    "test_data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reproduce feature generation\n",
    "\n",
    "feature_vectors_test = test_data.drop(['Formation', 'Well Name'], axis=1)\n",
    "feature_vectors_test = feature_vectors_test[['Depth','GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE']]\n",
    "\n",
    "df_diff_test = difference_vector(feature_vectors_test)    \n",
    "feature_vectors_test = diff_vec2frame(feature_vectors_test, df_diff_test)\n",
    "\n",
    "# Drop duplicated columns and column of zeros\n",
    "\n",
    "feature_vectors_test = feature_vectors_test.T.drop_duplicates().T   \n",
    "feature_vectors_test.drop('diffGRGR', axis = 1, inplace = True)\n",
    "\n",
    "# Create statistical feature differences using preivously caluclated mean and std values from train data.\n",
    "\n",
    "feature_vectors_test = feature_stat_diff(feature_vectors_test, frame_mean, frame_std)\n",
    "feature_vectors_test = feature_vectors_test[predictors]\n",
    "scaler = preprocessing.StandardScaler().fit(feature_vectors_test)\n",
    "scaled_features = scaler.transform(feature_vectors_test)\n",
    "\n",
    "predicted_random_forest = alg.predict(scaled_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_random_forest\n",
    "test_data['Facies'] = predicted_random_forest\n",
    "test_data.to_csv('test_data_prediction_CE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
